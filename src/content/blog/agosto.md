---
title: 'Inteligencia Artificial y Confianza'
description: 'Basado en el artículo publicado en la sección Opinión de Communications of the ACM'
pubDate: 'Aug 24 2025'
heroImage: '../../assets/agosto.png'
---

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 2px; border-radius: 12px; margin: 2rem 0;">
  <div style="background: white; padding: 2rem; border-radius: 10px;">
    <h3 style="color: #667eea; margin-top: 0; margin-bottom: 1.5rem; font-size: 1.25rem; border-bottom: 2px solid #667eea; padding-bottom: 0.5rem;">
        Información del Artículo Original
    </h3>
    
<div style="display: grid; gap: 1rem;">
      <div style="display: flex; gap: 0.75rem;">
        <span style="font-weight: 600; color: #4a5568; min-width: 120px;">Título:</span>
        <span style="color: #2d3748; font-style: italic;">AI and Trust</span>
      </div>
      
<div style="display: flex; gap: 0.75rem;">
        <span style="font-weight: 600; color: #4a5568; min-width: 120px;">Autor:</span>
        <span style="color: #2d3748;">Bruce Schneier</span>
      </div>
      
<div style="display: flex; gap: 0.75rem;">
        <span style="font-weight: 600; color: #4a5568; min-width: 120px;">Fecha:</span>
        <span style="color: #2d3748;">12 de Agosto 2025</span>
      </div>
      <div style="display: flex; gap: 0.75rem;">
        <span style="font-weight: 600; color: #4a5568; min-width: 120px;">Referencia:</span>
        <span style="color: #2d3748;">Schneier, B. (2025, agosto). <i>AI and Trust.</i> Communications of the ACM, 68(8).</span>
      </div>
    </div>
  </div>
</div>

---

El artículo de Bruce Schneier aborda un pilar ético fundamental para el desarrollo de la IA: el concepto de confianza. El autor argumenta de manera clara que la Inteligencia Artificial nunca podrá ser nuestro "amigo" porque solo es un servicio. El problema radica en que el diseño de las interfaces de IA nos lleva a confundir la confianza interpersonal con la confianza social. Esta última basada en la fiabilidad y previsibilidad, es la única que escala y debe ser garantizada por el gobierno.

Schneier sostiene que el diseño conversacional de muchas IAs nos incita a cometer un error de categoría, confundiendo el servicio impersonal con la amistad. Esta confusión es explotada por las empresas que controlan la tecnología, permitiendo que operen como "dobles agentes" que sirven tanto al usuario como a la agenda oculta del fabricante. La principal preocupación de seguridad no es la confidencialidad, sino la Integridad: garantizar que los datos y el modelo no han sido manipulados o sesgados, lo cual será el desafío de seguridad primario en la IA del futuro.

La explicación del autor concluye con un llamado al gobierno, señalando que su rol es crear y mantener la confianza social. Es responsabilidad del Estado regular y aplicar estándares mínimos de transparencia y seguridad para forzar el comportamiento de las empresas e incentivar una IA confiable. Sin este marco regulatorio, la confianza social se degradará rápidamente, comprometiendo la base ética de la adopción tecnológica.

Pienso que la idea del “doble agente” está muy bien planteada. Cuando hablamos con un chatbot de servicio al cliente sentimos que nos está ayudando pero en realidad puede estar recolectando información para la empresa que lo maneja. Eso nos obliga como profesionales a preguntarnos cuál es el verdadero propósito de las herramientas que construimos. Y también creo que dejar de ver solo la seguridad y empezar a hablar de integridad es un cambio importante. Al final, la IA tiene que tener el nivel de calidad suficiente como para que la sociedad realmente pueda confiar en ella.

En el caso de Costa Rica, siento que el tema de la regulación no puede esperar. Si el gobierno empieza a usar IA en servicios públicos o instituciones como la CCSS adoptan estas tecnologías de forma masiva, las personas tienen que tener motivos reales para confiar, no solo lo que diga la publicidad. Es preocupante que se pueda confundir la confianza interpersonal con la confianza social, porque eso podría abrirle la puerta a que empresas extranjeras influyan en cómo funcionan nuestros servicios públicos sin la supervisión que debería existir aquí.

Después de leer este artículo, me queda claro que el futuro responsable de la IA es un tema de gobernanza, no solo de tecnología. No podemos darnos el lujo de confiar ciegamente en sistemas de IA sin un marco legal que exija transparencia e integridad.

Y si realmente queremos aprovechar los beneficios y reducir los riesgos, creo que los desarrolladores deberíamos dedicar más tiempo a asegurar una trazabilidad real en los datos y modelos que son críticos, especialmente en los servicios esenciales para que siempre se pueda verificar que nadie los manipuló. Además de promover la transparencia, deberíamos apoyar las iniciativas de regulación que obliguen a las empresas a explicar el origen y las bases éticas de sus modelos. Eso ayudaría a construir la confianza social de la que habla Schneier.

---

### Referencia bibliográfica

Schneier, B. (2025, agosto). *AI and Trust.* Communications of the ACM, 68(8).